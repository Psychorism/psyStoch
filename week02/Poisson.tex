\documentclass[12pt]{article} 

\usepackage{geometry}
\geometry{a4paper} 

\usepackage{graphicx} 
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{float} 
\usepackage{wrapfig} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-2pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\linespread{1.2} 
\setlength{\parskip}{\baselineskip} % vertical spaces
\setlength\parindent{0pt} % remove all indentation from paragraphs


\usepackage{ntheorem}
\usepackage{mdframed}

\theoremstyle{nonumberbreak}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=10,%
rightmargin=10,
backgroundcolor=gray!20,%
innertopmargin=0pt,%
ntheorem]{theorem}{}




\begin{document}

\title{\textbf{Poisson Processes}}
\author{Hyunwoo Gu}
\date{}

\maketitle

%----------------------------------------------------------------------------------------
%   Section 0: Review
%----------------------------------------------------------------------------------------
\section*{Review: Renewal process}

\subsection*{Renewal processes}

A \textbf{renewal (counting) process} $\{N(t), t > 0\}$ is a \textbf{nonnegative integer-valued} stochastic process that registers the successive occurrences of an event during the time interval $(0, t]$, where the time durations between consecutive '' events " are positive, IID RVs. 

Let the successive occurence times between
events be $\{\xi_k\}_{k=1}^\infty$ such that $X_i$ is the elapsed time from the $(i-1)$st event until the occurrence of the ith event, where

$$
F_\xi (x) = P(\xi_k \le x)
$$

Note that $F_\xi(0) = 0$, which means $\xi_k$ is \textbf{positive} random variables, almost surely. 

Let the \textbf{partial sum process} $S_0 = 0$, $S_n = S_{n-1} + \xi_n$, where $\xi_i >0$ IID. $S_n$ is interpreted as the \textbf{waiting time} until the occurrence of $n$th event. 

\include 
graphics


Let $N_t := \mathrm{argmax}_k \{S_k \le t\}$, 

which is the \textbf{number of indices} $n$ for which $0 < S_n \le t$. 

In common practice, the counting process $\{N(t), t \ge 0 \}$ and the partial sum process $\{ S_n, n \ge 0 \}$ are interchangeably called the \textbf{renewal process}. 



$$
F \to \mathbb{E} N_t
$$

For $X \perp Y$, we have the \textbf{convolution}

$$
F_{X+Y}(x) = F_X \ast F_Y := \int_{\mathbb{R}} F_X(x-y) dF(y)
$$

\begin{theorem}
\textbf{Theorem}. For $S_n = S_{n-1} + \xi_n$, $\xi_i$ IID,
\begin{itemize}
	\item $u(t) = \sum_{n=1}^\infty F^{n\ast} (t) < \infty$
	\item $u(t) = \mathbb{E} N_t$
\end{itemize}
\end{theorem}

\subsection{Laplace transform}

For $f : \mathbb{R}_+ \to \mathbb{R}$, \textbf{Laplace transform} is defined as 

$$
\mathcal{L}_f(s) = \int_0^\infty e^{-sx} f(x) dx
$$

\begin{itemize}
	\item $f$ : density of $\xi$, then $\mathcal{L}_f(s) = m(s) = \mathbb{E}[e^{-s\xi}]$
	\item $\mathcal{L}_{f_1 \ast f_2} (s) = \mathcal{L}_{f_1} (s) \mathcal{L}_{f_2} (s)$ 
	\item $F$ : $\mathcal{L}_F(s) = \mathcal{L}_p(s)/s$
\end{itemize}

For the last property,

$$
LHS = -\int_{\mathbb{R}_+} F(x) \frac{d(e^{-sx})}{s} = - \frac{F(x)e^{-sx}}{s} \Vert_0^\infty + \frac{1}{s} \int_{\mathbb{R}_+}p(x) e^{-sx}dx = \frac{1}{s} \int_{\mathbb{R}_+}p(x) e^{-sx}dx
$$


Consider

$$
F \to \mathbb{E} N_t^{-}
$$

where 

$$
\begin{aligned}
\mathbb{E} N_t &= u(t) = \sum_{n=1}^\infty F^{n\ast} (t) \\[8pt]
&= F(t) + \sum_{n=1}^\infty F^{n\ast} (t) \ast F(t) \\[10pt]
u&= F + u \ast F = F + u \ast p
\end{aligned}
$$

where $\int_\mathbb{R} u(x-y) dF(y) = \int_\mathbb{R} u(x-y) p(y) dy$


Note that 

$$
\begin{aligned}
\mathcal{L}_u (s) &= \mathcal{L}_F (s) + \mathcal{L}_u (s) \cdot \mathcal{L}_p (s) \\[8pt]
&= \frac{\mathcal{L}_p(s)}{1-\mathcal{L}_p(s)}
\end{aligned}
$$


So we can follow 

\begin{itemize}
	\item $F \to \mathcal{L}_p$
	\item $\mathcal{L}_p \to \mathcal{L}_u$
	\item $\mathcal{L}_u \to u$
\end{itemize}

where the inverse Laplace transform can be obtained using Bromwich integral



For example, let $\{S_n \}_{n=1}^\infty$ to be

$$
S_n := S_{n-1} + \xi_n
$$

where $\xi_i \sim p(x) = e^{-x}/2 + e^{-2x}, x>0$. Then $\mathbb{E} (N_t^{-})?$


\textbf{$p \to \mathcal{L}(p)$}. 

$$
\begin{aligned}
\mathcal{L}(p) = \frac{1}{2(s+1)} + \frac{1}{s+2} = \frac{3s + 4}{2(s+1)(s+2)}  
\end{aligned}
$$




\pagebreak
%----------------------------------------------------------------------------------------
%   Section 1
%----------------------------------------------------------------------------------------
\section{Homogeneous Poisson processes}


\subsection{Defintion of Poisson process as a renewal process}


\begin{theorem}
\textbf{Theorem}
\begin{enumerate}[label=(\roman*)]
	\item $P_{S_n} (x) = \lambda \frac{(\lambda x)^{n-1}}{(n-1)!} e^{-\lambda x} \mathbf{1}_{x>0}$
	\item $P(N_t = n) = e^{-\lambda t} \frac{(\lambda t)^n}{n!}$
\end{enumerate}
\end{theorem}

\textbf{Proof} 

\textbf{I}

For $n=1$ case, $S_i = \xi_1$

$$
P_{S_1} (x) = \lambda e^{-\lambda x}, x>0
$$

Suppose $n=k$ case holds. For $n=k+1$, 

$$
\begin{aligned}
P_{S_{n+1}}(x) &= \int_0^x P_{S_n} ( x-y)  P_{S_n} ( x-y) \\[8pt]
&= \int_0^x \frac{}{} \\[8pt]
&= \frac{\lambda^{n+1}}{(n-1)!} 
\end{aligned}
$$




\textbf{II}

$$
(1 - e^{-\lambda t} \sum_{k=0}^{n-1} \frac{(\lambda t)^k}{k!}) - (1 - e^{-\lambda t} \sum_{k=0}^n \frac{(\lambda t)^k}{k!}) = e^{-\lambda t} \frac{(\lambda t)^n}{n!}
$$



\subsection{Memoryless property}


A random variable $X$ process 

$$
P \{ X > u + v \} = P \{ X > u  \} P \{ X > v \}
$$

if $P (X > v) > 0$, then

$$
P \{ X > u + v  | X > v\} = P\{ X > u\}
$$


DO NOT confuse with 


\begin{theorem}
\textbf{Theorem 2}. Let $X$ be a RV with density $p(x)$. Then

\begin{center}
$X$ : memoryless $\Leftrightarrow p(x) = \lambda e^{-\lambda x}$
\end{center}

\end{theorem}


(\textbf{Example}) Suppose busses arrive every $20 \pm 2$ minutes. This is \textbf{}







\subsection{Other definitions of Poisson processes}


\begin{theorem}
\textbf{Definition 2}. For $N_t$ : an integer-valued RV,

\begin{itemize}
	\item 0) $N_0 = 0$, a.s.
	\item $N_t$ has independent increments 
	\item $N_t$ has stationary increment 
\end{itemize}
\end{theorem}


Poisson process is widely used for describing the sum of odds. Note that


\begin{theorem}
\begin{itemize}
	\item $P\{ N_{t+h} - N_t = 0 \} = 1 - \lambda h + \bar{\bar{o}}(h)$
	\item $P\{ N_{t+h} - N_t = 1 \} = \lambda h \bar{\bar{o}}(h)$
	\item $P\{ N_{t+h} - N_t \ge 2 \} = \bar{\bar{o}}(h)$
\end{itemize}
\end{theorem}

\textbf{Proof} 

$$
\mathrm{lim}_{h \to 0} \frac{1 - P\{ N_{t+h} - N_t = 0 \}}{h} = \mathrm{lim}_{h \to 0} \frac{1-e^{-\lambda h}}{h} = \lambda
$$

by L'Hospital's rule.


\begin{theorem}
\textbf{Definition 3}. For $N_t$ : an integer-valued RV,
	\item 0) $N_0 = 0$, a.s.
	\item 1) $N_t$ has independent increments
	\item 2) $N_t$ has stationary increment 
	\item 3) $\mathrm{lim}_{h \to 0} \frac{P\{ N_{t+h} - N_t \ge 2 \}}{P\{ N_{t+h} - N_t = 1 \}} = 0 $
\end{theorem}


where $f(h) = \bar{\bar{o}} (g(h))$ means

$$
\mathrm{lim}_{h\to0} \frac{f(h)}{g(h)} = 0
$$ 

Note that 

$$
\frac{1 - P(N_{t+h} - N_t = 0) }{h} - \lambda = \bar{\bar{o}} (1) P\{ N_{t+h} - N_t = 0 \}
$$




\subsection{Limit theorems for renewal processes}


Consider a stochastic process $\{S_n \}_{n=1}^\infty$,

$$
S_n := S_{n-1} + \xi_n
$$

for IID $\xi_i > 0$. 

\begin{theorem}
\textbf{Theorem 1}. Assume $\mu := \mathbb{E} (\xi_1) < \infty$. Then 

$$
N_t / t \overset{t \to \infty}{\to} 1/\mu, a.s.
$$

This is analogous to \textbf{SLLN}, where 

$$
(\xi_1 + \cdots + \xi_n) / n \overset{t \to \infty}{\to}  \mu, a.s.
$$
\end{theorem}


\textbf{Proof} 

$$
\begin{aligned}
&S_{N_t} \le t \le S_{N_{t+1}} \\[8pt]
&\frac{N_t}{S_{N_{t+1}}} \le \frac{N_t}{t} \le \frac{N_t}{S_{N_t}} 
\end{aligned}
$$

where we have

$$
\begin{aligned}
&\lim_{t\to\infty} \frac{N_t}{S_{N_t}} = \lim_{n\to\infty} \frac{n}{S_n} = 1/\mu \\[8pt]
&\lim_{t\to\infty} \frac{N_t}{S_{N_{t+1}}} = 
\lim_{t\to\infty} \frac{N_t}{N_{t+1}} \frac{N_{t+1}}{S_{N_{t+1}}} = 1/\mu
\end{aligned}
$$


\begin{theorem}
\textbf{Theorem 2}. Assume $\sigma^2 = \mathrm{Var} (\xi_1) < \infty$. Then

$$
Z_t := \frac{N_t - t/\mu}{\sigma \sqrt{t} / \mu^{3/2}} \overset{d}{\to} N(0,1)
$$

This is analogous to \textbf{CLT}, where 

$$
\frac{\xi_1 + \cdots + \xi_n - n\mu}{\sigma \sqrt{n}} \overset{d}{\to}  N(0,1)
$$
\end{theorem}


\textbf{Proof} 

Note that 

$$
P \left( \frac{S_n - n\mu}{\sigma \sqrt{n}} \le x \right) \to P(x)
$$



\pagebreak
%----------------------------------------------------------------------------------------
%   Section 2
%----------------------------------------------------------------------------------------
\section{Models related to Poisson processes}

\subsection{Nonhomogeneous Poisson process}

\begin{itemize}
	\item $P\{ N_{t+h} - N_t = 0 \} = 1 - \lambda h + \bar{\bar{o}}(h)$
	\item $P\{ N_{t+h} - N_t = 1 \} = \lambda h \bar{\bar{o}}(h)$
	\item $P\{ N_{t+h} - N_t \ge 2 \} = \bar{\bar{o}}(h)$
\end{itemize}


\subsection{Relation between renewal theory and non-homogeneous Poisson processes}


\subsection{Elements of the queueing theory : M/G/k systems}

We can consider \textbf{queueing} as an example of \textbf{birth and death processes}. 

A queueing process is a process where customers arrive at some designated place where a service of some kind is being rendered(e.g. teller's window). It is assumed that the time between arrivals(\textbf{interarrival time}), and the time spent in providing service for a given
customer are governed by probabilistic laws . The length of the queue at a
given time t is represented by X(t)


Recalling the defining properties of the \textbf{homogeneous Poisson process}, 

\textbf{I. }

\begin{itemize}
	\item \textbf{M} : memoryless (Poisson)
	\item \textbf{D} : deterministic
	\item \textbf{G} : general
\end{itemize}


\textbf{II. }


For $N(t)$

\begin{itemize}
	\item $N_1(t)$, $\lambda_1 = \lambda (1 - G(\tau - t) )$ : \textbf{are still being served at} $t$
	\item $N_2(t)$, $\lambda_2 = \lambda G(\tau - t)$ : \textbf{are a }
\end{itemize}



Assume $M/G/\infty$. Note that both $N_1(t)$ and $N_2(t)$ are nonhomogeneous processes, because in homogeneous Poisson process

$$
\lambda(t) = \lambda = const
$$

which does not hold for either $N_1(t)$ or $N_2(t)$. 

$$
P\{ N_1(t) = n_1,  N_2(t) = n_2 \} = \underbrace{P\{ N_1(t) = n_1,  N_2(t) = n_2 | N(t) = n_1 + n_2 \}}{\binom{n_1+n_2}{n_1} (1 - G(\tau -t))^{n_1} G(\tau -t)^{n_2} } \cdot \underbrace{P\{ N(t) = n_1 + n_2 \}}{e^{-\lambda t} \frac{(\lambda t)^{n_1 + n_2}}{(n_1+n_2)!}}
$$



A \textbf{compound Poisson process}

$$
X_t = \sum_{k=1}^{N_t} \xi_k
$$

where $\xi: \phi_\xi (u) = \mathbb{E} [e^{iu\xi} ]$



\begin{theorem}
\textbf{Theorem}.

$$
\phi_{X_{\xi_t} - X_{\xi_s}} = e^{\lambda(t-s) \left( \phi_{\xi_1}(u) - 1 \right)}
$$

where $t > s \ge 0$

\textbf{Corollary}.


\end{theorem}


\textbf{Proof} 

Since both derivative and mathematical expectations are linear, we obtain that

$$
\begin{aligned}
\Phi^{(r)}(u) &= \mathbb{E}(i^r \xi^r e^{iu\xi}) \\[8pt]
\Phi^{(r)}(0) &= \mathbb{E}(i^r \xi^r e^{i\cdot 0 \xi}) = i^r \mathbb{E} (\xi^r)
\end{aligned}
$$

Thus 


Note that 

$$
\begin{aligned}
(LHS) &= \mathbb{E} \left[ e^{iu (X_t - X_s)} \right] \\[8pt]
&= \sum_{k=0}^\infty \mathbb{E} \left[ e^{iu(X_t - X_s)} \vert N_t - N_s = k \right] \\[8pt]
&= \sum_{k=0}^\infty \mathbb{E} \left[ e^{iu(X_t - X_s)}\right] \\[8pt]
&= P\left\{ N_t - N_S = k \right\} \\[8pt]
&= P\left\{ N_t - N_S = k \right\} 
\end{aligned}
$$







$N_t$ is the probability for 


Probability generating function(BGF)


For $\xi \ge 0$:

$$
\phi_\xi (u) = \mathbb{E}[u]
$$ 


Note that if $\xi_1 \perp \xi_2$ then  $\phi_{\xi_1+\xi_2}$




Moment 


A \textbf{characteristic function} is a functional that can be used for any random variable


\pagebreak
%----------------------------------------------------------------------------------------
%	Appendix
%----------------------------------------------------------------------------------------
\section*{Karlin: Renewal processes}
\setcounter{section}{0}


\section{Review: Basics \& Renewal Processes}



Let us consider \textbf{Poisson process} as an example of continous time Markov chains,

$$
\phi_t (w) = \mathbb{E} \{ e^{iwX(t)} \} = \sum_{n=0}^\infty \frac{e^{-\lambda t} (\lambda t)^n e^{iwn}}{n!} = exp[\lambda t (e^{iw} -1)]
$$

Thus 

$$
\mathbb{E} (X(t)) = \lambda t \ \ \mathbb{Var}(X(t)) = \lambda t
$$


\begin{theorem}
\textbf{Theorem 4.2.1}. The waiting times $T_k$ are IID following an exponential distribution with $\lambda$. 
\end{theorem}



\section{Poisson Processes: Continuous Time Markov Chains}


\subsection{Postulates for the Poisson process}


First, 

Notice that the right-hand side is independent of $x$. 

$$
\begin{aligned}
P\{ X(t+h) - X(t) = 1 | X(t) = x \} &= \lambda h + o(h), h \to 0_+, (x=0,1,2,\cdots) \\[10pt]
\Leftrightarrow \mathrm{lim}_{h \to 0+} \frac{P\{ X(t+h) - X(t) = 1 | X(t) = x \}}{h} = \lambda
\end{aligned}
$$



Second


$$
P\{ X(t+h) - X(t) = 0 | X(t) = x \} = 1 - \lambda h + o(h), h \to 0_+
$$


Third,

$$
X(0) = 0 
$$


\subsection{Pure birth process}

A natural generalization of the Poisson process is to permit the chance of
an event occurring at a given instant of time to depend upon the number of events which have already occurred. For example, \textbf{the probability of a birth at a given instant is proportional to the population size at that time}, which is known as the \textbf{Yule process}.




The characteristic function of $S_n$ is given by

$$
\phi_n(w) = \mathbb{E}(exp(iwS_n)) = \prod_{k=0}^{n-1} \mathbb{E}(exp(iw T_k)) = \prod_{k=0}^{n-1} \frac{\lambda_k}{\lambda_k - iw}
$$


\subsection{Yule process}



$$
\begin{aligned}
f_N(s) &= [f(s)]^N \\[8pt]
&= \left[ \frac{se^{-\beta t}}{1 - (1-e^{-\beta t})s} \right]^N \\[8pt]
&= (s e^{-\beta t})^N \sum_{m=0}^infty 
\end{aligned}
$$




\begin{theorem}
\textbf{Theorem 2.1}. The waiting times $T_k$ are independent and indentically distributed following an exponential distribution with parameter $\lambda$. 
\end{theorem}



\begin{theorem}
\textbf{Theorem 2.2}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}

\textbf{Proof}. ($\Rightarrow$) 




\begin{theorem}
\textbf{Theorem 2.3}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}

\textbf{Proof}. ($\Rightarrow$) 




\subsection{Birth and Death Processes}

To generalize the pure birth processes, we can permit $X(t)$ to decrease as well as increase, for example, by the death of members. This can be regarded as the continuous time analogs of random walks. 


\begin{itemize}
	\item $P_{i, i+1}(h) = \lambda_i h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{i, i-1}(h) = \mu_i h + o(h), h \to 0_+$, $i \ge 1$
	\item $P_{i,I} (h) = 1 -(\lambda_i + \mu_i) h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{ij} (0) = \delta_{ij}$
	\item $\mu_0 = 0$, $\lambda_0 > 0, \mu_i, \lambda_i > 0, i=1,2,\cdots$
\end{itemize}

The matrix $A$, the \textbf{infinitesimal generator} of the process,

$$
A = \begin{bmatrix}
-\lambda_0 & \lambda_0 & 0 & 0 & \cdots \\
\mu_1 & -(\lambda_1 + \mu_1)  & \lambda_1 & 0 & \cdots \\
0 & \mu_2 & -(\lambda_2 + \mu_2) & \lambda_2 & \cdots \\
0 & 0 & \mu_3 & -(\lambda_3 + \mu_3) & \cdots
\vdots & \vdots &  \vdots &  \vdots &   & 
\end{bmatrix}
$$



\subsection{Differential Equations of Birth and Death Processes}




\section{Problems}

(4.2.) Assume a device fails when a cumulative effect of $k$ shocks occur. If the shocks happen 

$$
f(t) = \begin{cases}
\frac{\lambda^k t^{k-1} e^{-\lambda t}}{\Gamma(k)} & t>0 \\[8pt]
0 & t \le 0
\end{cases}
$$


(4.6.) Let $X(t)$ be a homogeneous Poisson process with parameter $\lambda$. Determine the covariance between $X(t)$ and $X(t + \tau)$ where $t > 0$ and $\tau > 0$, i.e. compute

$$
\mathbb{E} \left[ \left( X(t) - \mathbb{E}(X(t)) \right) \left( X(t+\tau) - \mathbb{E}(X(t+\tau)) \right) \right]
$$



(4.9.) Let $X(t)$ be a pure birth continuous time Markov chain. Assume that 

$$
\begin{aligned}
P(event \in (t, th) | X(t) = odd) &= \lambda_1 h + o(h) \\[8pt]
P(event \in (t, th) | X(t) = odd) &= \lambda_1 h + o(h) \\[8pt]
\end{aligned}
$$


For the single-server process with $\lambda < \mu$ the stationary distribution is 

$$
\pi_n = \frac{\lambda_0 \lambda_1 \cdots \lambda_{n-1} }{\mu_1 \mu_2 \cdots \mu_n} = (\frac{\lambda}{\mu})^n
$$

which, when normalized, results in

$$
P_n = \frac{\mu - \lambda}{\mu} (\frac{\lambda}{\mu})^n, \ \ n \ge 0
$$


If the process has been going on a long time and $\lambda < \mu$, the probability of being served immediately upon arrival is 

$$
P_0 = (1 - \frac{\lambda}{\mu})
$$


If an arriving customer finds $n$ people in front of her, her total waiting time $T$, including his own service time, is the sum of service times of herself and those ahead, all distributed exponentially with param $\mu$, thus 

$$
\begin{aligned}
T | n \ \ \mathrm{ahead} &\equiv Gamma(n+1, \mu) \\[8pt]
\Leftrightarrow \ \ P \{ T \le t | n \ \ \mathrm{ahead}  \} &= \int_0^t \frac{\mu^{n+1} \tau^n e^{-\mu t} }{\Gamma(n+1)} d\tau \\[10pt]
\therefore \ \ P \{ T \le t\} &= \sum_{n=0}^\infty P \{ T \le t | n \ \ \mathrm{ahead} \} \cdot (\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})
\end{aligned}
$$  

since $(\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})$ is the probability that in the stationary case a customer on arrival will find $n$ ahead in line. 


$$
\begin{aligned}
P \{ T \le t\} &= \\[8pt]
\end{aligned}
$$



$$
\begin{aligned}
M(t) &= \sum_{j=0}^\infty j P_{ij}(t) \\[8pt]
M'(t) &= \lambda - \mu M(t) \\[8pt]
M(t) &= \frac{\lambda}{\mu} (1 - e^{-\mu t}) + i e^{-\mu t} \\[8pt]
\end{aligned}
$$

If we let $t \to \infty$, then $M(t) \to \lambda/\mu$, which is the mean value of the stationary distribution given above. 





\end{document}