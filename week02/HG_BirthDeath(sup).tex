\documentclass[12pt]{article} 

\usepackage{geometry}
\geometry{a4paper} 

\usepackage{graphicx} 
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{float} 
\usepackage{wrapfig} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-2pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\linespread{1.2} 
\setlength{\parskip}{\baselineskip} % vertical spaces
\setlength\parindent{0pt} % remove all indentation from paragraphs


\usepackage{ntheorem}
\usepackage{mdframed}

\theoremstyle{nonumberbreak}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=10,%
rightmargin=10,
backgroundcolor=gray!20,%
innertopmargin=0pt,%
ntheorem]{theorem}{}




\begin{document}

\title{\textbf{Birth and Death Processes(Sup)}}
\author{Hyunwoo Gu}
\date{}

\maketitle

%----------------------------------------------------------------------------------------
%	Supplementary: Birth and Death Processes
%----------------------------------------------------------------------------------------
\section{Birth-death Markov Chains}(6.4. from \textit{Gallager})

A \textit{birth-death Markov chain} is a Markov chain where the state space is the set of nonnegative integers. A transition from $i$ to $i+1$ is regarded as a birth and $i+1$ to $i$ is regarded as a death. Let us denote $p_i := P_{i,i+1}$ and $q_i := P_{i,i-1}$, thus $P_{ii} = 1 - p_i - q_i$. So what is the steady-state probabilities of these BD chains?

Note that the number of transitions from $i$ to $i+1$ differs by at most $1$ from the number of transitions 


Thus if we visualize renewal-reward process with renewals on occurrences of state $i$ and unit reward on transitions from $i$ to $i+1$, the limiting time-average number of transitions per unit time is $\pi_i p_i$. Similarly, the limiting time-average number of transitions per unit time from $i+1$ to $i$ is $\pi_{i+1} q_{i+1}$. Thus by equating in limit,

$$
\pi_i p_i = \pi_{i+1} q_{i+1}, \ \ \forall i\ge 0
$$


It is convenient to define $\rho_i$ as $p_i / q_{i+1}$. Then we have $\pi_{i+1} = \rho_i \pi_i$, and iterating this,

$$
\pi_i = \pi_0 \prod_{j=0}^{i-1} \rho_j, \ \ \pi_0 := \frac{1}{1 + \sum_{i=1}^\infty \prod_{j=0}^{i-1} \rho_j}
$$

If $\sum_{i=1}^\infty \prod_{j=0}^{i-1} \rho_j < \infty$, then $\pi_0$ is positive and \textbf{all the states are positive recurrent}. If this sum of products is infinite, then no state is positive recurrent. 


\section{General pure birth processes}


\subsection{Postulates for the Poisson process}

In order to define more general processes of a similar kind, let us point out various further properties that the Poisson possesses. In particular, it is a Markov process on the nonnegative integers which has the following properties :


$$
\begin{aligned}
P\{ X(t+h) - X(t) = 1 | X(t) = x \} &= \lambda h + o(h), h \to 0_+, (x=0,1,2,\cdots) \\[10pt]
\Leftrightarrow \mathrm{lim}_{h \to 0+} \frac{P\{ X(t+h) - X(t) = 1 | X(t) = x \}}{h} = &\lambda
\end{aligned}
$$

Notice that the right-hand side is independent of $x$. 


$$
P\{ X(t+h) - X(t) = 0 | X(t) = x \} = 1 - \lambda h + o(h), h \to 0_+
$$

$$
X(0) = 0 
$$


\subsection{Pure birth process}

A natural generalization of the Poisson process is to permit the chance of an event occurring at a given instant of time to depend upon the number of events which have already occurred. For example, \textbf{the probability of a birth at a given instant is proportional to the population size at that time}, which is known as the \textbf{Yule process}.


The characteristic function of $S_n$ is given by

$$
\phi_n(w) = \mathbb{E}(exp(iwS_n)) = \prod_{k=0}^{n-1} \mathbb{E}(exp(iw T_k)) = \prod_{k=0}^{n-1} \frac{\lambda_k}{\lambda_k - iw}
$$


\subsection{Yule process}

The \textbf{Yule process} is an example of a \textbf{pure birth process} that arises in physics and biology. Assume that each member in a population has a probability $\beta h + o(h)$ of giving birth to a new member in an interval of time length $h (\beta > 0)$. Furthermore assume that there are $X(O) = N$ members present at time $0$. Assuming independence and no interaction among members of the population, the binomial theorem gives

$$
\begin{aligned}
Pr[X(t+h) - X(t) = 1 | X(t) = n] &= \binom{n}{1} \left[ \beta h + o(h) \right] \left( 1 - \beta h + o(h) \right)^{n-1} \\[8pt]
&= n\beta h + o_n(h)
\end{aligned}
$$

where $\lambda_n = n \beta$. Thus

$$
P_n'(t) = -\beta [nP_n(t) - (n-1) P_{n-1}(t) ], \ \ n=1,2,\cdots
$$

under BV : $P_1(0)=1, P_n(0)=0, \ \ n=2,3,\cdots$. 

The solution is 

$$
P_n(t) = e^{-\beta t} (1 - e^{-\beta t})^{n-1}
$$

The generating function is 

$$
\begin{aligned}
f(x) &= \sum_{n=1}^\infty P_n(t) s^n \\[8pt]
&= s e^{-\beta t} \sum_{n=1}^\infty \left[ (1-e^{-\beta t}) s \right]^{n-1} \\[8pt]
&= \frac{se^{-\beta t}}{ (1 - (1-e^{-\beta t})s) }
\end{aligned}
$$

Letting $P_{N_n}(t) := Pr[X(t) = n | X(0) = N]$ and $f_N(s) = \sum_{n=N}^\infty P_{N_n}(t)s^n$, we have


$$
\begin{aligned}
f_N(s) &= [f(s)]^N \\[8pt]
&= \left[ \frac{se^{-\beta t}}{1 - (1-e^{-\beta t})s} \right]^N \\[8pt]
&= (s e^{-\beta t})^N \sum_{m=0}^infty \binom{m+N-1}{m} (1-e^{-\beta t})^m s^m
\end{aligned}
$$



$T$ : the waiting time of $X(t)$ in the state $i$. 


Letting $G_i(t) := P(T_i \ge t)$, 

$$
\begin{aligned}
G_i (t+h) &= G_i(t) G_i(h) = G_i(t) \left[ P_{ii}(h) + o(h) \right] \\[8pt]
&= G_i(t) [ 1- (\lambda_i + \mu_i)h ] + o(h) \\[10pt]
\Leftrightarrow \frac{ G_i(t+h) - G_i(t)  }{ h } = -(\lambda_i + \mu_i) G_i(t) + o(1) \\[8pt]
G_i'(t) &= -(\lambda_i + \mu_i) G_i(t)
\end{aligned}
$$

where the last line corresponds to the IVP of with $G_i(0) = 1$




\subsection{Birth and Death Processes}

To generalize the pure birth processes, we can permit $X(t)$ to decrease as well as increase, for example, by the death of members. This can be regarded as the continuous time analogs of random walks. 


\begin{itemize}
	\item $P_{i, i+1}(h) = \lambda_i h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{i, i-1}(h) = \mu_i h + o(h), h \to 0_+$, $i \ge 1$
	\item $P_{i,I} (h) = 1 -(\lambda_i + \mu_i) h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{ij} (0) = \delta_{ij}$
	\item $\mu_0 = 0$, $\lambda_0 > 0, \mu_i, \lambda_i > 0, i=1,2,\cdots$
\end{itemize}

The matrix $A$, the \textbf{infinitesimal generator} of the process,

$$
A = \begin{bmatrix}
-\lambda_0 & \lambda_0 & 0 & 0 & \cdots \\
\mu_1 & -(\lambda_1 + \mu_1)  & \lambda_1 & 0 & \cdots \\
0 & \mu_2 & -(\lambda_2 + \mu_2) & \lambda_2 & \cdots \\
0 & 0 & \mu_3 & -(\lambda_3 + \mu_3) & \cdots
\vdots & \vdots &  \vdots &  \vdots &   & 
\end{bmatrix}
$$


\subsection{Differential Equations of Birth and Death Processes}

As in the pure birth and Poisson processes, the transition probabilities $P_{ij}$ satisfy a system of differential equations, known as \textit{Kolmogorov differential equations}. 

$$
\begin{aligned}
P_{0j}'(t) &= -\lambda_0 P_{0j} (t) + \lambda_0 P_{ij}(t) \\[8pt]
P_{ij}'(t) &= \mu_i P_{i-1, j} (t) - (\lambda_i + \mu_i) P_{ij}(t) + \lambda_i P_{i+1, j}(t) \\[8pt]
\end{aligned}
$$

where the boundary condition $P_{ij}(0) = \delta_{ij}$. 



\subsection{Examples of Birth and Death Processes}

\subsubsection{Linear Growth with Immigration}

A birth and death process is called a \textbf{linear growth process} if $\lambda_n = \lambda_n + a$ and $\mu_n = \mu n$ with $\lambda, \mu, a >0$. Such 







\subsubsection{Queueing}





\subsection{Birth and Death Processes with Absorbing States}

\textbf{Probability of absorption into state $0$}


$$
\mathbf{u}_i = \frac{\lambda_i}{\mu_i + \lambda_i} \mathbf{u}_{i+1}  + \frac{\mu_i}{\mu_i + \lambda_i} \mathbf{u}_{i-1}
$$

where $u_0 = 1$. 


\textbf{Probability of absorption into state $0$}



\begin{theorem}
\textbf{Theorem 7.1.}. Consider a BDP with birth and death parameters $\lambda_n$ and $\mu_n$ with $n \ge 1$, where $\lambda_0 = 0$ so that $0$ is an absorbing state. 

\textbf{The probability of absorption into state $0$ from the intial state $m$} is given as 

$$
\begin{cases}
\frac{ \sum_{i=m}^\infty \left( \prod_{j=1}^i \mu_j/\lambda_j \right)  }{ 1+ \sum_{i=1}^\infty \left( \prod_{j=1}^i \mu_j/\lambda_j \right)} & \sum_{i=1}^\infty \left( \prod_{j=1}^i \mu_j/\lambda_j \right) < \infty \\
1 & \sum_{i=1}^\infty \left( \prod_{j=1}^i \mu_j/\lambda_j \right) = \infty
\end{cases}
$$

The \textbf{mean time to absorption} is



\end{theorem}

\textbf{Proof}. ($\Rightarrow$) 



\subsection{Finite State Continuous Time Markov Chains}






For the single-server process with $\lambda < \mu$ the stationary distribution is 

$$
\pi_n = \frac{\lambda_0 \lambda_1 \cdots \lambda_{n-1} }{\mu_1 \mu_2 \cdots \mu_n} = (\frac{\lambda}{\mu})^n
$$

which, when normalized, results in

$$
P_n = \frac{\mu - \lambda}{\mu} (\frac{\lambda}{\mu})^n, \ \ n \ge 0
$$


If the process has been going on a long time and $\lambda < \mu$, the probability of being served immediately upon arrival is 

$$
P_0 = (1 - \frac{\lambda}{\mu})
$$


If an arriving customer finds $n$ people in front of her, her total waiting time $T$, including his own service time, is the sum of service times of herself and those ahead, all distributed exponentially with param $\mu$, thus 

$$
\begin{aligned}
T | n \ \ \mathrm{ahead} &\equiv Gamma(n+1, \mu) \\[8pt]
\Leftrightarrow \ \ P \{ T \le t | n \ \ \mathrm{ahead}  \} &= \int_0^t \frac{\mu^{n+1} \tau^n e^{-\mu t} }{\Gamma(n+1)} d\tau \\[10pt]
\therefore \ \ P \{ T \le t\} &= \sum_{n=0}^\infty P \{ T \le t | n \ \ \mathrm{ahead} \} \cdot (\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})
\end{aligned}
$$  

since $(\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})$ is the probability that in the stationary case a customer on arrival will find $n$ ahead in line. 


$$
\begin{aligned}
P \{ T \le t\} &= \\[8pt]
\end{aligned}
$$



$$
\begin{aligned}
M(t) &= \sum_{j=0}^\infty j P_{ij}(t) \\[8pt]
M'(t) &= \lambda - \mu M(t) \\[8pt]
M(t) &= \frac{\lambda}{\mu} (1 - e^{-\mu t}) + i e^{-\mu t} \\[8pt]
\end{aligned}
$$

If we let $t \to \infty$, then $M(t) \to \lambda/\mu$, which is the mean value of the stationary distribution given above. 







\end{document}