\documentclass[12pt]{article} 

\usepackage{geometry}
\geometry{a4paper} 

\usepackage{graphicx} 
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{float} 
\usepackage{wrapfig} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-2pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\linespread{1.2} 
\setlength{\parskip}{\baselineskip} % vertical spaces
\setlength\parindent{0pt} % remove all indentation from paragraphs


\usepackage{ntheorem}
\usepackage{mdframed}

\theoremstyle{nonumberbreak}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=10,%
rightmargin=10,
backgroundcolor=gray!20,%
innertopmargin=0pt,%
ntheorem]{theorem}{}




\begin{document}

\title{\textbf{Markov Chains, Gaussian Processes, \\ \& Stationarity}}
\author{Hyunwoo Gu}
\date{}

\maketitle


%----------------------------------------------------------------------------------------
%   Section 1
%----------------------------------------------------------------------------------------
\begin{center}
\section*{I. Markov Chains}
\end{center}
\setcounter{section}{1}

A discrete time Markov chain $\{ X_n \}$ is a Markov stochastic process whose state space is a countable or finite set. 

\bigskip

\begin{theorem}
\textbf{Definition}. The \textbf{Markov chain} $\{X_n\}$ is a stochastic process such that

$$
P(X_n = j | X_{n-1} = i_{n-1}, \cdots, X_0 = i_0) = P(X_n = j | X_{n-1} = i_{n-1})
$$

for any $i_0, \cdots, i_{n-1}, j \in \mathcal{S}$, the state space. 
\end{theorem}


We have

$$
\begin{aligned}
P\left[ X_0 = i_0, \cdots, X_n = i_n \right] &= P[X_n = i_n | X_0 = i_0, \cdots, X_{n-1} = i_{n-1}] \\[8pt]
&\cdot P(X_0 = i_0, \cdots, X_{n-1} = i_{n-1}) \\[8pt]
&= P(X_n = i_n | X_{n-1} = i_{n-1}) \cdots P(X_1 = i_1 | X_0 = i_0) P(X_0 = i_0)
\end{aligned}
$$


\pagebreak
\textbf{Chapman-Kolmogorov Equation}


\begin{theorem}
\textbf{Definition}. A stochastic process $\{X_t \}_a^b$ is said to satisfy the Markov property if for any $a \le t_1 < \cdots < t_n < t \le b$, the equality 

$$
P(X_t \le x | X_{t_1}, \cdots, X_{t_n} ) = P(X_t \le x | X_{t_n})
$$

holds for any $x \in \mathbb{R}$, or equivalently, the equality

$$
P(X_t \le x | X_{t_i} = y_i, i=1,\cdots,n ) = P(X_t \le x | X_{t_n} = y_n)
$$

holds for any $y_i \in \mathbb{R}$. 

\textbf{Lemma}. Suppose a stochastic process $X_t$, $a \le t \le b$ is adapted to a filtration $\{\mathcal{F}_t : a \le t \le b \}$ and satisfies the condition

$$
P (X_t \le x | \mathcal{F}_s) = P(X_t \le x | X_s), \forall s < t, x\in \mathbb{R}
$$

then $X_t$ is a Markov process
\end{theorem}


\textbf{Proof}. Let $t_1 < t_2 < \cdots < t_n < t$ and $x \in \mathbb{R}$. Then 

$$
\begin{aligned}
P(X_t \le x | X_{t_1}, \cdots, X_{t_n} ) &= \mathbb{E} \left[ P(X_t \le x | \mathcal{F}_{t_n}) | X_{t_1}, \cdots, X_{t_n} \right] \\[8pt]
&= \mathbb{E} \left[ P(X_t \le x | X_{t_n}) | X_{t_1}, \cdots, X_{t_n} \right]
&= P(X_t \le x | X_{t_n})
\end{aligned}
$$

Define the conditional probability $P_{s,x}(t, dy) := P(X_t \in dy | X_s = x)$ a \textbf{transition probability} of a Markov process $X_t$. 


\begin{theorem}
\textbf{Definition}. The equality is called \textbf{Chapman-Kolmogorov equation}:

$$
P_{s,x} (t,A) = \int_{-\infty}^\infty P_{u,z}(t,A) P_{s,x}(u,dz)
$$

for all $s < u < t$, $x \in \mathbb{R}$, and $A \in \mathcal{B}(\mathbb{R})$, the Borel field of $\mathbb{R}$. 

This is due to 

$$
\begin{aligned}
P(X_{t_1} \le c_1, \cdots, X_{t_n} \le c_n) &= \int_{-\infty}^{c_1} \cdots \int_{-\infty}^{c_n} P_{t_{n-1}, x_{n-1}} (t_n, dx_n) \\[8pt]
&\times \cdots P_{t_1, x_1} (t_2, dx_2) \nu(dx_1)
\end{aligned}
$$

\end{theorem}


\subsection{Examples of Markov chains}


\subsubsection*{Spatially homogeneous Markov chains}

Let $\xi_1, \xi_2, \cdots$ IID samples such that $P(\xi = i) = a_i$. Let $\eta_n := \sum_{i=1}^n \xi_i$

$$
P = \begin{bmatrix}
a_0 & a_1 & a_2 & a_3 & \cdots \\
0 & a_0 & a_1 & a_2 & \cdots \\
0 & 0 & a_0 & a_1 &  \cdots \\
\vdots & & & & \\
\end{bmatrix}
$$

Note that

$$
P(X_{n+1} = j | X_n \ i) = \begin{cases} a_{j-i} & j \ge i \\ 0 & j < i \end{cases}
$$


\subsubsection*{One-dimensional random walks}

$$
P = \begin{bmatrix}
r_0 & p_0 & 0 & 0 & \cdots \\
q_1 & r_1 & p_1 & 0 & \cdots \\
0 & q_2 & r_2 & p_2 & \cdots \\
\vdots & & & & \\
\end{bmatrix}
$$

Note that $P(X_{n+1} = i+1 | X_n = i) = p_i$, $P(X_{n+1} = i-1 | X_n = i) = q_i$, $P(X_{n+1} = i | X_n = i) = r_i$


\subsubsection*{Gambler's ruin}

Let 

\begin{itemize}
	\item $N - i$ : Casino's initial wealth
	\item $i$ : Gambler's initial wealth
\end{itemize}

For $S := \{ 0,1,\cdots, N \}$

$$
\begin{aligned}
P_{i,i+1} &= p, P_{i,i-1} = 1-p \\[8pt]
P_{0,0} &= 1, P_{N,N} = 1
\end{aligned}
$$

We have the state spaces 

$$S = \{ 0\} \cup \{ 1, \cdots, N-1\} \cup \{ N\}$$

\subsubsection*{Two-dimensional random walks}

\subsubsection*{Three-dimensional random walks}

\subsubsection*{Success runs}

\subsubsection*{Branching processes}



\subsection{Classifications of states of a Markov chains}


\subsubsection*{Accessibility}

State $j$ is \textbf{accessible} from state $i$ if $\exists n \ge 0$ such that $P^n_{ij} > 0$. Two states $i$ and $j$, each accessible to the other, are saide to \textbf{communicate}. If $i$ and $j$ do not communicate, then either

$$
P^n_{ij} = 0, \forall n \ge 0 \ \ or \ \ P^n_{ji} = 0, \forall n \ge 0 
$$

The properties of \textbf{communicaiton} as an \textbf{equivalence relation}:

\begin{itemize}
	\item (\textbf{Reflexivity}) : a consequence from the definition of $P^0_{ij} = \delta_{ij}$
	\item (\textbf{Symmetry}) : a consequence from the definition
	\item (\textbf{Transitivity})
\end{itemize}

(\textbf{Proof of transitivity}). $i \leftrightarrow j$ and $j \leftrightarrow k$ imply that there exist $n,m \in \mathbb{N}$ such that $P_{ij}^n >0$ and $P_{jk}^m >0$. Thus 

$$
P_{ij}^{n+m} = \sum_{r=0}^\infty P_{ir}^n P_{rk}^m \ge P_{ij}^n P_{jk}^m > 0
$$

And the similar argument shows the opposite way. 

We can say all the states are equivalent if there is no inner loop comming back to the state with probability 1. 


\subsubsection*{Periodicity}

\textbf{Period} of state $i$, $d(i)$ is the greatest common divisor(GCD) of all integers $n \ge 1$ where $P^n_{ii} > 0$, i.e.

$$
d(i) \equiv GCD \{ n : P_{ii}^n > 0 \}
$$

If $d(i) = 1$, then the state $i$ is called \textbf{aperiodic}. 

$$
P = \begin{bmatrix}
0 & 1 & 0 & 0 & \cdots & 0 \\
0 & 0 & 1 & 0 & \cdots & 0 \\
\vdots &  &  &  &  & \vdots \\
0 & 0 & 0 & 0 & \cdots & 1 \\
1 & 0 & 0 & 0 & \cdots & 0 \\
\end{bmatrix}
$$



\begin{theorem}
\textbf{Theorem 4.1}. Periodicity is a \textbf{class property}, i.e, if $i \leftrightarrow j$ then

$$
d(i) = d(j)
$$

\textbf{Theorem 4.2.}. If state $i$ has period $d(i)$ then there exists an integer $N$ depending on $i$ such that $\forall n \ge N$,

$$
P_{ii}^{nd(i)} > 0
$$

which asserts that a return to state i can occur at all sufficiently large multiples of the period $d(i)$. 
\end{theorem}



\subsubsection*{Recurrence}

Let us define

$$
f_{ii}^n := P(X_n = i, X_{n-1} \neq i, \cdots, X_1 \neq 1 | X_0 = i)
$$

Note that 

$$
P_{ii}^n := \sum_{k=0}^n f_{ii}^k P_{ii}^{n-k}
$$

where $f_{ii}^0 := 0$. 


\begin{theorem}
\textbf{Definition}. The \textbf{generating function} $P_{ij}(s)$ of the sequence $\{ P_{ij}^n \}$ is 

$$
P_{ij} (s) = \sum_{n=0}^\infty P_{ij}^n s^n, \forall |s|<1
$$

In a similar manner, the generating function of the sequence $\{ f_{ij}^n \}$ is 

$$
F_{ij}(s) = \sum_{n=0}^\infty f_{ij}^n s^n, \forall |s|
$$
\end{theorem}


We say a state $i$ is \textbf{recurrent} if and only if $\sum_{n=1}^\infty f_{ii}^n = 1$. i.e. it has \textbf{finite first return time}, a.s., where the first return time can be defined 

$$
\tau_{ii} := \begin{cases} min(n \ge 1 : X_n = i | X_0 =i) \\ \infty & P( X_n = i | X_0 =i) = 0 \end{cases}
$$

where $\sum_{n=1}^\infty f_{ii}^n = Pr(\tau_{ii} < \infty)$. 



\begin{theorem}
\textbf{Theorem 5.1.}. A state $i$ is recurrent if and only if 

$$
\sum_{n=1}^\infty P_{ii}^n = \infty
$$
\end{theorem}


\textbf{Proof} Assume $i$ is recurrent, that is, $\sum_{n=1}^\infty f_{ii}^n = 1$. Then by Lemma 5.1,

$$
\mathrm{lim}_{s \to 1-} \sum_{n=0}^\infty f_{ii}^n s^n = \mathrm{lim}_{s \to 1-} F_{ii}(s) = 1
$$

Thus using the fact that 

$$
\mathrm{lim}_{s \to 1-} P_{ii}(s) = \mathrm{lim}_{s \to 1-} \sum_{n=0}^\infty P_{ii}^n s^n = \infty
$$


\begin{theorem}
\textbf{Corollary 5.1.}. If $i \leftrightarrow j$ and if $i$ is recurrent then $j$ is recurrent.
\end{theorem}

i.e. \textbf{recurrence is the class property}.



\begin{theorem}
\textbf{Fact 1}. In irreducible MCs, all states are either \textbf{recurrent} or \textbf{transient}. 

\textbf{Fact 2}. In \textbf{finite}, irreducible MCs, all states are either \textbf{recurrent}. 
\end{theorem}

For example, 

\begin{itemize}
	\item An asymmetric 1D random walk is \textbf{transient}, since there is a probability of absorption 
	\item A symmetric 1D random walk is \textbf{aperiodic}, since it has period 2.
	\item In \textbf{gambler's ruin}, the states are \textbf{irreducible}
\end{itemize}



\subsection{Ergodic theorem}

\begin{theorem}
\textbf{Ergodic theorem}. Let $X_t$ irreducible, ergodic(recurrent and aperiodic) Markov chain. Then

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$
\end{theorem}



\begin{theorem}
\textbf{Corollary1}. $\pi P = \pi$

\textbf{Corollary2}. $\mathrm{lim}_{n\to\infty} P(X_n = j) = \pi_j$

\end{theorem}

where \textbf{corollary2} is equivalent to 

$$
\mathrm{lim}_{n\to\infty} \frac{1}{n}\sum_{i=1}^n I(X_k=j | X_0=j) = \pi_j
$$


We have the following additional theorem:


\begin{theorem}
\textbf{Theorem 1.1.}. Let $\{a_k \}$, $\{u_k \}$, $\{b_k \}$ be sequences indexed by $k=0, \pm 1, \pm 2, \cdots$ that the GCD of the integer $k$ for which $a_k >0$ is 1. If the renewal equation, for $n = 0, \pm 1, \pm 2, \cdots$

$$
u_n - \sum_{k=-\infty}^\infty a_{n-k} u_k = b_n
$$

is satisfied by a bounded sequence $\{ u_n\}$ of real numbers, then 

$$
\exists \mathrm{lim}_{n\to \infty} u_n, \exists \mathrm{lim}_{n\to -\infty} u_n
$$. Furthermore, if

$$
\mathrm{lim}_{n\to - \infty} u_n = 0
$$

then 

$$
\mathrm{lim}_{n\to \infty} u_n
$$

\end{theorem}





\subsection*{Quizzes}

\textbf{(Quiz 1)}. Find the stationary distribution of the following $P$:

$$
P = \begin{bmatrix}
0 & 1/2 & 0 & 0 & 1/2 \\
0 & 0 & 1 & 0 & 0 \\
1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\
0 & 1/2 & 0 & 0 & 1/2 \\
0 & 1/2 & 1/2 & 0 & 0 \\
\end{bmatrix}
$$

\textbf{(Answer)} The left-eigenvector corresponding to eigenvalue $1$ is as follows:

$$
(1/12, 3/12, 5/12, 1/12, 2)
$$



\textbf{(Quiz 2)}. Jane and Peter are playing chess. For Jane, the probabilities of wining, drawing, and losing a game number $t$ are $(w,d,l)$. Peter is slightly more emotional.

\begin{itemize}
	\item If he wins in the previous game, then $(w+\epsilon, d, l-\epsilon)$
	\item If he draws in the previous game, then $(w, d, l)$
	\item If he loses in the previous game, then $(w-\epsilon, d, l+\epsilon)$
\end{itemize}

Find the condition which guarantees that the probability of wining in stationry distribution for Peter is larger than that for Jane.


\textbf{(Answer)} Note that 

$$
\begin{aligned}
P_{Jane} &= \begin{bmatrix}
w & d & l \\
w & d & l \\
w & d & l \\
\end{bmatrix}
P_{Peter} &= \begin{bmatrix}
w + \epsilon& d & l-\epsilon \\
w & d & l \\
w - \epsilon & d & l+\epsilon \\
\end{bmatrix}
\end{aligned}
$$

It is direct that $Rank(P_{Jane}^T) = 1$, thus the multiplicity of $\lambda = 0$ is at least $2$. Note that $\lambda =1$ is also an eigenvalue, where the corresponding eigenvector is $(w,d,l)$. 

For $P_{Peter}^T$, we need some algebra to get

$$
x_1 = \frac{w(1-\epsilon) - l\epsilon}{1-2\epsilon}
$$

then by $w < x_1$, we have

$$
l < w
$$



\textbf{(Quiz 3)}. 

Is the following $P$ \textbf{ergodic}?

$$
P = \begin{bmatrix}
0 & 1/2 & 0 & 1/2 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1/2 & 0 & 1/2 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$$


\textbf{(Answer)} Note that it is \textbf{irreducible} and \textbf{recurrent}, but not \textbf{aperiodic}.


\textbf{(Quiz 4)}. 

Tell the number of equivalence classes, and all the periodic states of the following transition matrix:

$$
P = \begin{bmatrix}
1/3 & 1/3 & 1/3 & 0 \\
1/2 & 1/2 & 0 & 0 \\
1/4 & 1/4 & 0 & 1/2 \\
0 & 1/2 & 0 & 1/2 \\
\end{bmatrix}
$$

\textbf{(Answer)} Note that it is \textbf{irreducible} and \textbf{recurrent}. Note that $P_{11} > 0$, thus all the states are aperiodic. 


\textbf{(Quiz 5)}. 

Assume that there is a series of integer numbers, in which numbers $0,1,\cdots, 9$ appear randomly and independently of each other with equal probabilities. Let $x_n$ be a quantity of different numbers in $n$ first elements of the series. Find a stationary distribution of this chain.

\textbf{(Answer)}

$$
P = \begin{bmatrix}
1/9 & 8/9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 2/9 & 7/9 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 3/9 & 6/9 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 4/9 & 5/9 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 5/9 & 4/9 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 6/9 & 3/9 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 7/9 & 2/9 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 8/9 & 1/9 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$

Clearly the left eigenvector corresponding to $\lambda:=1$ is $(0, \cdots, 1)$. 


\pagebreak
%----------------------------------------------------------------------------------------
%   Section 2
%----------------------------------------------------------------------------------------
\begin{center}
\section*{II. Gaussian processes}
\end{center}
\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Random vector}



\subsection{Brownian motion}

\begin{center}
Brownian motion($B_t$) = Wiener process ($W_t$)
\end{center}


\begin{theorem}
\textbf{Proposition}. 

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}





\subsection*{Quizzes}

\textbf{(Quiz 1)}. Let $X_t$ be a Brownian motion. Find

$$
K(t,s) - Var(X_{min(t,s)})
$$


\textbf{(Answer)} Trivially $0$, since

$$
Var(X_{min(t,s)}) = min(t,s)
$$



\textbf{(Quiz 2)}. Let $Y_{n+1} := a Y_n + X_n$, where $n=0,1,\cdots$, $Y_0 := 0, |a| < 1$, $X_0, X_1, \cdots \overset{iid}{\sim} N(0,1)$. Find $cov(Y_4, Y_3)$. 


\textbf{(Answer)} Note that 

$$
\begin{aligned}
Cov(Y_4, Y_3) &= cov(a^3 X_0 + a^2 X_1 + a X_2 + X_3, a^2 X_0 + a X_1 + X_2) \\[8pt]
&= a^5 + a^3 + a
\end{aligned}
$$




\textbf{(Quiz 3)}. 


$$
K(t,s) - Var(X_{min(t,s)})
$$


\textbf{(Answer)} Trivially $0$, since

$$
Var(X_{min(t,s)}) = min(t,s)
$$




\pagebreak
%----------------------------------------------------------------------------------------
%   Section 3
%----------------------------------------------------------------------------------------
\begin{center}
\section*{III. Stationarity and Linear Filters}
\end{center}
\setcounter{section}{3}
\setcounter{subsection}{0}


\subsection{Spectral density of a wide-sense stationary process}


\textbf{Bocher}

$\phi(u): \mathbb{R} \to \mathbb{C}, \phi(u) = \mathbb{E} [e^{iu\xi}]$ : characteristic function, iff

\begin{itemize}
	\item 1) $\phi$ is constant
	\item 2) $\phi$ is positive semidefinite, $\sum_{j=1}^n z_j \bar{z}_k \phi(u_j - u_k) \ge 0$, $\forall (z_1, \cdots, z_n) \in \mathbb{C}^n, \forall (u_1, \cdots, u_n) \in \mathbb{R}^n$.
	\item 3) $\phi(0) = 1$
\end{itemize}

If 1), 2) are met, we have 

$$
\exists \mu: \phi(\mu) = \int e^{iux} \mu (dx)
$$






$X_t$ : weakly stationary. 

$$
\begin{aligned}
\sigma &: K(t,s) = \sigma(t-s) \\[8pt]
\end{aligned}
$$

if $\sigma$ is constant, and $\int |\sigma (u)| du < \infty $ 

$$
\mathcal{F}
$$


For example, there does not exist a stochastic process with the covariance $K(t,s) := sin( \lambda (t-s) )$, since it is not positive semi-definite.


$$
g(x) := 
$$


\textbf{Example 1)} $WN(0, \sigma^2)$

Note that 

$$
\begin{aligned}
\gamma(u) &= \sigma^2 1_{\{ u = 0 \}} \\[8pt]
g(x) &= \frac{\sigma^2}{2\pi}
\end{aligned}
$$



\textbf{Example 2)} $MA(1)$



\begin{theorem}
\textbf{Proposition}. 

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}





\subsection{Stochastic integration}






\subsection*{Quizzes}

\textbf{(Quiz 1)}. Let $Y_n$ be a stochastic process, such that

$$
Y_{n+1} := \alpha Y_n + X_n
$$

for $n=0, 1, \cdots$. Assume $Y_0 := 0, |\alpha| < 1$ and $X_n$ : a sequence of IID standard normal RVs. Determine whether $Y_n$ is stationary and find its mean and variance.


\textbf{(Answer)} $\forall t > s$

$$
\begin{aligned}
K(t,s) &= Cov(\alpha^{t-1} X_0 + \cdots \alpha^0 X_{t-1}, \alpha^{s-1} X_0 + \cdots + \alpha^0 X_{s-1}) \\[8pt]
&= \alpha^{t-1} \alpha^{s-1} + \alpha^{t-2} \alpha^{s-2} + \cdots + \alpha^{t-s} \\[8pt]
&= \alpha^{t-s} \frac{1-\alpha^{2s}}{1-\alpha^2}
\end{aligned}
$$

where $K(t,s)$ also depends on $s$, not only on $t-s$.




\textbf{(Quiz 2)}. Let $W_t$ be a Brownian motion and define $X_t := (1-t) W_{t/(1-t)}$, for $t \in (0,1)$. Is $X_t$ stationary? 

\textbf{(Answer)} No,

$$
\begin{aligned}
K(t,s) &= (1-t)(1-s) Cov(W_{t/(1-t)}, W_{s/(1-s)}) \\[8pt]
&= (1-t)(1-s) Cov(W_{t/(1-t)} - W_{s/(1-s)} + W_{s/(1-s)}, W_{s/(1-s)} - W_0) \\[8pt]
&= (1-t)(1-s) Var(W_{s/(1-s)}) = s(1-t)
\end{aligned}
$$

which is not weakly stationary. 


\textbf{(Quiz 3)}. Let $X_t$ be a process with independent and stationry increments and $\exists h>0$. Moreover, $\mathbb{E}(X_t) = 0, \mathbb{E}|X_t|^2 < \infty$. Is $Y_t = X_{t+h} - X_t$ a wide-sense stationary process? 

\textbf{(Answer)} Yes,

Note that if increments of a process is stationry, then the process is stationary.



\textbf{(Quiz 4)}. Let the autocovariance function of some stochastic process $X_t$ be $\gamma_X(u) := \begin{cases} 3 & u=0 \\ 1 & u=\pm2 \\ 0 & o.w. \end{cases}$ Find the \textbf{spectral density} of $Y_t := 3X_t + 2X_{t-1} + X_{t-2}$. 

\textbf{(Answer)} Note that 

$$
\begin{aligned}
g_X(u) &= \frac{1}{2\pi} ( 3 + e^{-2iu} + e^{2iu}) \\[8pt]
&= \frac{1}{2\pi} (3 + 2 cos(2u)) \\[10pt]
g_Y(u) &= g_X(u) |\mathcal{F}[\rho] (u)|^2
\end{aligned}
$$

where $\rho(h) = := \begin{cases} 3 & h=0 \\ 2 & h=1 \\ 1 & h=2 \\ 0 & o.w. \end{cases}$. Therefore

$$
\begin{aligned}
\mathcal{F}[\rho] (u) &= e^{2iu} + 2e^{iu} + 3 \\[8pt]
[\mathcal{F}[\rho] (u)]^2 &= \mathcal{F} \times \bar{\mathcal{F}} \\[8pt]
&= (e^{2iu} + 2e^{iu} + 3) \times (e^{-2iu} + 2e^{-iu} + 3) \\[8pt]
&= 9 + 4 + 1 + 8(e^{iu} + e^{-iu}) + 3(e^{2iu} + e^{-2iu}) \\[8pt]
&= 14 + 3\cdot 2cos(2u) + 8\cdot 2 cos(u) \cdot g_Y(u) \\[8pt]
&= \frac{1}{2\pi} (3 + 2cos(2u)) (14 + 3 \cdot 2cos(2u) + 8\cdot 2 cos(u)) \\[8pt]
\end{aligned}
$$



\textbf{(Quiz 4)}. Let the autocovariance function of some stochastic process $X_t$ be $\gamma_X(u) := \begin{cases} 3 & u=0 \\ 1 & u=\pm2 \\ 0 & o.w. \end{cases}$ Find the \textbf{spectral density} of $Y_t := 3X_t + 2X_{t-1} + X_{t-2}$. 

\textbf{(Answer)} Note that for $t > s + h$, we have

$$
K(t,s) = Cov(W_{t+h} - W_t, W_{s+h} - W_s) = 0
$$

For $t \le s + h$, we have

$$
\begin{aligned}
K(t,s) &= Cov(W_{t+h} - W_t, W_{s+h} - W_s) \\[8pt]
&= Cov(W_{t+h} - W_{s+h} + W_{s+h} - W_t, W_{s+h} - W_s) \\[8pt]
&= Cov(W_{s+h} - W_t, W_{s+h} - W_t + W_t - W_s) \\[8pt]
&= Var(W_{s+h} - W_t) = h - |t - s|
g_Y(u) &= g_X(u) |\mathcal{F}[\rho] (u)|^2
\end{aligned}
$$





\end{document}