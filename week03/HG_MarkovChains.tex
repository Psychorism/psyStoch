\documentclass[12pt]{article} 

\usepackage{geometry}
\geometry{a4paper} 

\usepackage{graphicx} 
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{float} 
\usepackage{wrapfig} 

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-2pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\linespread{1.2} 
\setlength{\parskip}{\baselineskip} % vertical spaces
\setlength\parindent{0pt} % remove all indentation from paragraphs


\usepackage{ntheorem}
\usepackage{mdframed}

\theoremstyle{nonumberbreak}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=10,%
rightmargin=10,
backgroundcolor=gray!20,%
innertopmargin=0pt,%
ntheorem]{theorem}{}




\begin{document}

\title{\textbf{Markov Chains, Gaussian Processes, \\ \& Stationarity}}
\author{Hyunwoo Gu}
\date{}

\maketitle


%----------------------------------------------------------------------------------------
%   Section 1
%----------------------------------------------------------------------------------------
\begin{center}
\section*{I. Markov Chains}
\end{center}
\setcounter{section}{1}

A discrete time Markov chain $\{ X_n \}$ is a Markov stochastic process whose state space is a countable or finite set. 



\begin{theorem}
\textbf{Definition}. The \textbf{generating function} $P_{ij}(s)$ of the sequence $\{ P_{ij}^n \}$ is 

$$
P_{ij} (s) = \sum_{n=0}^\infty P_{ij}^n s^n, \forall |s|<1
$$

In a similar manner, the generating function of the sequence $\{ f_{ij}^n \}$ is 

$$
F_{ij}(s) = \sum_{n=0}^\infty f_{ij}^n s^n, \forall |s|
$$
\end{theorem}

From the property 


\begin{theorem}
\textbf{Theorem 5.1.}. A state $i$ is recurrent if and only if 

$$
\sum_{n=1}^\infty P_{ii}^n = \infty
$$
\end{theorem}


\textbf{Proof} Assume $i$ is recurrent, that is, $\sum_{n=1}^\infty f_{ii}^n = 1$. Then by Lemma 5.1,

$$
\mathrm{lim}_{s \to 1-} \sum_{n=0}^\infty f_{ii}^n s^n = \mathrm{lim}_{s \to 1-} F_{ii}(s) = 1
$$

Thus using the fact that 




\subsection{Ergodic theorem}

\begin{theorem}
\textbf{Ergodic theorem}. Let $X_t$ ergodic Markov chain, i.e. irreducible, recurrent, aperiodic. Then

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}



\begin{theorem}
\textbf{Corollary1}.

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}



\textbf{Proof} 

\textbf{I}

For $i=1, \cdots, M$,


$$
\begin{aligned}
\left( \pi^\ast P \right)_i &= \sum_{j=1}^M \pi_j^\ast \\[8pt]
\end{aligned}
$$


where note that 

$$
\begin{aligned}
\left( P^{(n+1)} \right)_{ki} &= p_{ki} (n+1) = \sum{j=1}^M p_{kj} (n) p_{ji} \\[10pt]
\therefore \mathrm{lim}_{n \to \infty} p_{ki} (n+1) = \mathrm{lim}_{n \to \infty} p_{ki} (n) = \pi_i^\ast
\end{aligned}
$$



$$
P_{S_1} (x) = \lambda e^{-\lambda x}, x>0
$$

Suppose $n=k$ case holds. For $n=k+1$, 

$$
\begin{aligned}
P_{S_{n+1}}(x) &= \int_0^x P_{S_n} ( x-y)  P_{S_n} ( x-y) \\[8pt]
&= \int_0^x \frac{}{} \\[8pt]
&= \frac{\lambda^{n+1}}{(n-1)!} 
\end{aligned}
$$




\textbf{II}

$$
(1 - e^{-\lambda t} \sum_{k=0}^{n-1} \frac{(\lambda t)^k}{k!}) - (1 - e^{-\lambda t} \sum_{k=0}^n \frac{(\lambda t)^k}{k!}) = e^{-\lambda t} \frac{(\lambda t)^n}{n!}
$$







\subsection{Examples of Markov chains}


\subsubsection{Spatially homogeneous Markov chains}

Let



\subsubsection{One-dimensional random walks}

$$
P = \begin{bmatrix}

\end{bmatrix}
$$









\subsubsection{Success runs}



\subsubsection{Branching processes}

$$
P[\xi = k] = a_k
$$





\begin{theorem}
\textbf{Theorem 1.1.}. Let $\{a_k \}$, $\{u_k \}$, $\{b_k \}$ be sequences indexed by $k=0, \pm 1, \pm 2, \cdots$ that the GCD of the integer $k$ for which $a_k >0$ is 1. If the renewal equation, for $n = 0, \pm 1, \pm 2, \cdots$

$$
u_n - \sum_{k=-\infty}^\infty a_{n-k} u_k = b_n
$$

is satisfied by a bounded sequence $\{ u_n\}$ of real numbers, then 

$$
\exists \mathrm{lim}_{n\to \infty} u_n, \exists \mathrm{lim}_{n\to -\infty} u_n
$$. Furthermore, if

$$
\mathrm{lim}_{n\to - \infty} u_n = 0
$$

then 

$$
\mathrm{lim}_{n\to \infty} u_n
$$

\end{theorem}


\textbf{Chapman-Kolmogorov Equation}



\textbf{Martingales}





\subsection{Classifications of states of a Markov chains}


\subsubsection{Accessibility}

State $j$ is \textbf{accessible} from state $i$ if $\exists n \ge 0$ such that $P^n_{ij} > 0$, i.e. 

If two states $i$ and $j$ do not communicate, then 

$$
P^n_{ij} = 0, \forall n \ge 0 \ \ or \ \ P^n_{ji} = 0, \forall n \ge 0 
$$

The definition of \textbf{communicaiton}: 

\begin{itemize}
	\item (\textbf{Reflexivity})
	\item (\textbf{Symmetry})
	\item (\textbf{Transitivity})
\end{itemize}



\subsubsection{Periodicity}

\textbf{Period} of state $i$, $d(i)$ is the greatest common divisor(GCD) of all integers $n \ge 1$ where $P^n_{ii} > 0$. 

$$
P = \begin{bmatrix}
0 & 1 & 0 & 0 & \cdots & 0 \\
0 & 0 & 1 & 0 & \cdots & 0 \\
\vdots &  &  &  &  & \vdots \\
0 & 0 & 0 & 0 & \cdots & 1 \\
1 & 0 & 0 & 0 & \cdots & 0 \\
\end{bmatrix}
$$


\begin{theorem}
\textbf{Theorem 1}. Periodicity is a \textbf{class property}, i.e, if $i \leftrightarrow j$ then

$$
d(i) = d(j)
$$
\end{theorem}




\subsubsection{Recurrence}


\begin{theorem}
\textbf{Corollary 5.1.}. If $i \leftrightarrow j$ and if $i$ is recurrent then $j$ is recurrent.
\end{theorem}






\subsection*{Quizzes}

\textbf{(Quiz 1)}. Find the stationary distribution of the following $P$:

$$
P = \begin{bmatrix}
0 & 1/2 & 0 & 0 & 1/2 \\
0 & 0 & 1 & 0 & 0 \\
1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\
0 & 1/2 & 0 & 0 & 1/2 \\
0 & 1/2 & 1/2 & 0 & 0 \\
\end{bmatrix}
$$

\textbf{(Answer)} The left-eigenvector corresponding to eigenvalue $1$ is as follows:

$$
(1/12, 3/12, 5/12, 1/12, 2)
$$



\textbf{(Quiz 2)}. Jane and Peter are playing chess. For Jane, the probabilities of wining, drawing, and losing a game number $t$ are $(w,d,l)$. Peter is slightly more emotional.

\begin{itemize}
	\item If he wins in the previous game, then $(w+\epsilon, d, l-\epsilon)$
	\item If he draws in the previous game, then $(w, d, l)$
	\item If he loses in the previous game, then $(w-\epsilon, d, l+\epsilon)$
\end{itemize}

Find the condition which guarantees that the probability of wining in stationry distribution for Peter is larger than that for Jane.


\textbf{(Answer)} Note that 

$$
\begin{aligned}
P_{Jane} &= \begin{bmatrix}
w & d & l \\
w & d & l \\
w & d & l \\
\end{bmatrix}
P_{Peter} &= \begin{bmatrix}
w + \epsilon& d & l-\epsilon \\
w & d & l \\
w - \epsilon & d & l+\epsilon \\
\end{bmatrix}
\end{aligned}
$$

It is direct that $Rank(P_{Jane}^T) = 1$, thus the multiplicity of $\lambda = 0$ is at least $2$. Note that $\lambda =1$ is also an eigenvalue, where the corresponding eigenvector is $(w,d,l)$. 

For $P_{Peter}^T$, we need some algebra to get

$$
x_1 = \frac{w(1-\epsilon) - l\epsilon}{1-2\epsilon}
$$

then by $w < x_1$, we have

$$
l < w
$$



\textbf{(Quiz 3)}. 


$$
K(t,s) - Var(X_{min(t,s)})
$$


\textbf{(Answer)} Trivially $0$, since

$$
Var(X_{min(t,s)}) = min(t,s)
$$





\pagebreak
%----------------------------------------------------------------------------------------
%   Section 2
%----------------------------------------------------------------------------------------
\begin{center}
\section*{II. Gaussian processes}
\end{center}
\setcounter{section}{2}
\setcounter{subsection}{0}

\subsection{Random vector}



\subsection{Brownian motion}

\begin{center}
Brownian motion($B_t$) = Wiener process ($W_t$)
\end{center}


\begin{theorem}
\textbf{Proposition}. 

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}





\subsection*{Quizzes}

\textbf{(Quiz 1)}. Let $X_t$ be a Brownian motion. Find

$$
K(t,s) - Var(X_{min(t,s)})
$$


\textbf{(Answer)} Trivially $0$, since

$$
Var(X_{min(t,s)}) = min(t,s)
$$



\textbf{(Quiz 2)}. Let $Y_{n+1} := a Y_n + X_n$, where $n=0,1,\cdots$, $Y_0 := 0, |a| < 1$, $X_0, X_1, \cdots \overset{iid}{\sim} N(0,1)$. Find $cov(Y_4, Y_3)$. 


\textbf{(Answer)} Note that 

$$
\begin{aligned}
Cov(Y_4, Y_3) &= cov(a^3 X_0 + a^2 X_1 + a X_2 + X_3, a^2 X_0 + a X_1 + X_2) \\[8pt]
&= a^5 + a^3 + a
\end{aligned}
$$




\textbf{(Quiz 3)}. 


$$
K(t,s) - Var(X_{min(t,s)})
$$


\textbf{(Answer)} Trivially $0$, since

$$
Var(X_{min(t,s)}) = min(t,s)
$$




\pagebreak
%----------------------------------------------------------------------------------------
%   Section 3
%----------------------------------------------------------------------------------------
\begin{center}
\section*{III. Stationarity and Linear Filters}
\end{center}
\setcounter{section}{3}
\setcounter{subsection}{0}


\subsection{Spectral density of a wide-sense stationary process}


\textbf{Bocher}

$\phi(u): \mathbb{R} \to \mathbb{C}, \phi(u) = \mathbb{E} [e^{iu\xi}]$ : characteristic function, iff

\begin{itemize}
	\item 1) $\phi$ is constant
	\item 2) $\phi$ is positive semidefinite, $\sum_{j=1}^n z_j \bar{z}_k \phi(u_j - u_k) \ge 0$, $\forall (z_1, \cdots, z_n) \in \mathbb{C}^n, \forall (u_1, \cdots, u_n) \in \mathbb{R}^n$.
	\item 3) $\phi(0) = 1$
\end{itemize}

If 1), 2) are met, we have 

$$
\exists \mu: \phi(\mu) = \int e^{iux} \mu (dx)
$$






$X_t$ : weakly stationary. 

$$
\begin{aligned}
\sigma &: K(t,s) = \sigma(t-s) \\[8pt]
\end{aligned}
$$

if $\sigma$ is constant, and $\int |\sigma (u)| du < \infty $ 

$$
\mathcal{F}
$$


For example, there does not exist a stochastic process with the covariance $K(t,s) := sin( \lambda (t-s) )$, since it is not positive semi-definite.


$$
g(x) := 
$$


\textbf{Example 1)} $WN(0, \sigma^2)$

Note that 

$$
\begin{aligned}
\gamma(u) &= \sigma^2 1_{\{ u = 0 \}} \\[8pt]
g(x) &= \frac{\sigma^2}{2\pi}
\end{aligned}
$$



\textbf{Example 2)} $MA(1)$



\begin{theorem}
\textbf{Proposition}. 

$$
\begin{aligned}
\exists \mathrm{lim}_{n\to\infty} P_{ij}(n) =& \pi_j^\ast > 0 \\[8pt]
\sum_{j=1}^M \pi_j^\ast &= 1
\end{aligned}
$$

\end{theorem}





\subsection{Stochastic integration}






\subsection*{Quizzes}

\textbf{(Quiz 1)}. Let $Y_n$ be a stochastic process, such that

$$
Y_{n+1} := \alpha Y_n + X_n
$$

for $n=0, 1, \cdots$. Assume $Y_0 := 0, |\alpha| < 1$ and $X_n$ : a sequence of IID standard normal RVs. Determine whether $Y_n$ is stationary and find its mean and variance.


\textbf{(Answer)} $\forall t > s$

$$
\begin{aligned}
K(t,s) &= Cov(\alpha^{t-1} X_0 + \cdots \alpha^0 X_{t-1}, \alpha^{s-1} X_0 + \cdots + \alpha^0 X_{s-1}) \\[8pt]
&= \alpha^{t-1} \alpha^{s-1} + \alpha^{t-2} \alpha^{s-2} + \cdots + \alpha^{t-s} \\[8pt]
&= \alpha^{t-s} \frac{1-\alpha^{2s}}{1-\alpha^2}
\end{aligned}
$$

where $K(t,s)$ also depends on $s$, not only on $t-s$.




\textbf{(Quiz 2)}. Let $W_t$ be a Brownian motion and define $X_t := (1-t) W_{t/(1-t)}$, for $t \in (0,1)$. Is $X_t$ stationary? 

\textbf{(Answer)} No,

$$
\begin{aligned}
K(t,s) &= (1-t)(1-s) Cov(W_{t/(1-t)}, W_{s/(1-s)}) \\[8pt]
&= (1-t)(1-s) Cov(W_{t/(1-t)} - W_{s/(1-s)} + W_{s/(1-s)}, W_{s/(1-s)} - W_0) \\[8pt]
&= (1-t)(1-s) Var(W_{s/(1-s)}) = s(1-t)
\end{aligned}
$$

which is not weakly stationary. 


\textbf{(Quiz 3)}. Let $X_t$ be a process with independent and stationry increments and $\exists h>0$. Moreover, $\mathbb{E}(X_t) = 0, \mathbb{E}|X_t|^2 < \infty$. Is $Y_t = X_{t+h} - X_t$ a wide-sense stationary process? 

\textbf{(Answer)} Yes,

Note that if increments of a process is stationry, then the process is stationary.



\textbf{(Quiz 4)}. Let the autocovariance function of some stochastic process $X_t$ be $\gamma_X(u) := \begin{cases} 3 & u=0 \\ 1 & u=\pm2 \\ 0 & o.w. \end{cases}$ Find the \textbf{spectral density} of $Y_t := 3X_t + 2X_{t-1} + X_{t-2}$. 

\textbf{(Answer)} Note that 

$$
\begin{aligned}
g_X(u) &= \frac{1}{2\pi} ( 3 + e^{-2iu} + e^{2iu}) \\[8pt]
&= \frac{1}{2\pi} (3 + 2 cos(2u)) \\[10pt]
g_Y(u) &= g_X(u) |\mathcal{F}[\rho] (u)|^2
\end{aligned}
$$

where $\rho(h) = := \begin{cases} 3 & h=0 \\ 2 & h=1 \\ 1 & h=2 \\ 0 & o.w. \end{cases}$. Therefore

$$
\begin{aligned}
\mathcal{F}[\rho] (u) &= e^{2iu} + 2e^{iu} + 3 \\[8pt]
[\mathcal{F}[\rho] (u)]^2 &= \mathcal{F} \times \bar{\mathcal{F}} \\[8pt]
&= (e^{2iu} + 2e^{iu} + 3) \times (e^{-2iu} + 2e^{-iu} + 3) \\[8pt]
&= 9 + 4 + 1 + 8(e^{iu} + e^{-iu}) + 3(e^{2iu} + e^{-2iu}) \\[8pt]
&= 14 + 3\cdot 2cos(2u) + 8\cdot 2 cos(u) \cdot g_Y(u) \\[8pt]
&= \frac{1}{2\pi} (3 + 2cos(2u)) (14 + 3 \cdot 2cos(2u) + 8\cdot 2 cos(u)) \\[8pt]
\end{aligned}
$$



\textbf{(Quiz 4)}. Let the autocovariance function of some stochastic process $X_t$ be $\gamma_X(u) := \begin{cases} 3 & u=0 \\ 1 & u=\pm2 \\ 0 & o.w. \end{cases}$ Find the \textbf{spectral density} of $Y_t := 3X_t + 2X_{t-1} + X_{t-2}$. 

\textbf{(Answer)} Note that for $t > s + h$, we have

$$
K(t,s) = Cov(W_{t+h} - W_t, W_{s+h} - W_s) = 0
$$

For $t \le s + h$, we have

$$
\begin{aligned}
K(t,s) &= Cov(W_{t+h} - W_t, W_{s+h} - W_s) \\[8pt]
&= Cov(W_{t+h} - W_{s+h} + W_{s+h} - W_t, W_{s+h} - W_s) \\[8pt]
&= Cov(W_{s+h} - W_t, W_{s+h} - W_t + W_t - W_s) \\[8pt]
&= Var(W_{s+h} - W_t) = h - |t - s|
g_Y(u) &= g_X(u) |\mathcal{F}[\rho] (u)|^2
\end{aligned}
$$






\pagebreak
%----------------------------------------------------------------------------------------
%	Appendix
%----------------------------------------------------------------------------------------
\section*{Karlin: Markov Chains}
\setcounter{section}{0}


By definition, 


$$
\begin{aligned}
P\left[ X_0 = i_0, \cdots, X_n = i_n \right] &= P[X_n = i_n | X_0 = i_0, \cdots, X_{n-1} = i_{n-1}] \\[8pt]
&\cdot P(X_0 = i_0, \cdots, X_{n-1} = i_{n-1}) \\[8pt]
&= 
\end{aligned}
$$


\section{Examples of Markov Chains}

\subsection{Spatially Homogeneous Markov Chains}


\subsection{One-dimensional random walks}




\subsection{Three-dimensional random walks}



\subsection{A Discrete Queing Markov Chain}




\begin{theorem}
\textbf{Theorem 2.2}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}


\begin{theorem}
\textbf{Theorem 2.2}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}



\section{Review: Basics \& Renewal Processes}



Let us consider \textbf{Poisson process} as an example of continous time Markov chains,

$$
\phi_t (w) = \mathbb{E} \{ e^{iwX(t)} \} = \sum_{n=0}^\infty \frac{e^{-\lambda t} (\lambda t)^n e^{iwn}}{n!} = exp[\lambda t (e^{iw} -1)]
$$

Thus 

$$
\mathbb{E} (X(t)) = \lambda t \ \ \mathrm{Var}(X(t)) = \lambda t
$$


\section{Poisson Processes: Continuous Time Markov Chains}


\subsection{Postulates for the Poisson process}


First, 

Notice that the right-hand side is independent of $x$. 

$$
\begin{aligned}
P\{ X(t+h) - X(t) = 1 | X(t) = x \} &= \lambda h + o(h), h \to 0_+, (x=0,1,2,\cdots) \\[10pt]
\Leftrightarrow \mathrm{lim}_{h \to 0+} \frac{P\{ X(t+h) - X(t) = 1 | X(t) = x \}}{h} = \lambda
\end{aligned}
$$



Second


$$
P\{ X(t+h) - X(t) = 0 | X(t) = x \} = 1 - \lambda h + o(h), h \to 0_+
$$


Third,

$$
X(0) = 0 
$$


\subsection{Pure birth process}

A natural generalization of the Poisson process is to permit the chance of
an event occurring at a given instant of time to depend upon the number of events which have already occurred. For example, \textbf{the probability of a birth at a given instant is proportional to the population size at that time}, which is known as the \textbf{Yule process}.




The characteristic function of $S_n$ is given by

$$
\phi_n(w) = \mathbb{E}(exp(iwS_n)) = \prod_{k=0}^{n-1} \mathbb{E}(exp(iw T_k)) = \prod_{k=0}^{n-1} \frac{\lambda_k}{\lambda_k - iw}
$$


\subsection{Yule process}



$$
\begin{aligned}
f_N(s) &= [f(s)]^N \\[8pt]
&= \left[ \frac{se^{-\beta t}}{1 - (1-e^{-\beta t})s} \right]^N \\[8pt]
&= (s e^{-\beta t})^N \sum_{m=0}^infty 
\end{aligned}
$$




\begin{theorem}
\textbf{Theorem 2.1}. The waiting times $T_k$ are independent and indentically distributed following an exponential distribution with parameter $\lambda$. 
\end{theorem}



\begin{theorem}
\textbf{Theorem 2.2}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}

\textbf{Proof}. ($\Rightarrow$) 




\begin{theorem}
\textbf{Theorem 2.3}. Let $F(x)$ a distribution such that $F(0) = 0$ and $F(x) < 1$ for some $x >0 $, then $F(x)$ is an exponential distribution iff

$$
F(x+y) - F(y) = F(x)[1-F(y)], \forall x,y \ge 0
$$
\end{theorem}

\textbf{Proof}. ($\Rightarrow$) 




\subsection{Birth and Death Processes}

To generalize the pure birth processes, we can permit $X(t)$ to decrease as well as increase, for example, by the death of members. This can be regarded as the continuous time analogs of random walks. 


\begin{itemize}
	\item $P_{i, i+1}(h) = \lambda_i h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{i, i-1}(h) = \mu_i h + o(h), h \to 0_+$, $i \ge 1$
	\item $P_{i,I} (h) = 1 -(\lambda_i + \mu_i) h + o(h), h \to 0_+$, $i \ge 0$
	\item $P_{ij} (0) = \delta_{ij}$
	\item $\mu_0 = 0$, $\lambda_0 > 0, \mu_i, \lambda_i > 0, i=1,2,\cdots$
\end{itemize}

The matrix $A$, the \textbf{infinitesimal generator} of the process,

$$
A = \begin{bmatrix}
-\lambda_0 & \lambda_0 & 0 & 0 & \cdots \\
\mu_1 & -(\lambda_1 + \mu_1)  & \lambda_1 & 0 & \cdots \\
0 & \mu_2 & -(\lambda_2 + \mu_2) & \lambda_2 & \cdots \\
0 & 0 & \mu_3 & -(\lambda_3 + \mu_3) & \cdots
\vdots & \vdots &  \vdots &  \vdots &   & 
\end{bmatrix}
$$


\section{Problems}

(4.2.) Assume a device fails when a cumulative effect of $k$ shocks occur. If the shocks happen 

$$
f(t) = \begin{cases}
\frac{\lambda^k t^{k-1} e^{-\lambda t}}{\Gamma(k)} & t>0 \\[8pt]
0 & t \le 0
\end{cases}
$$


(4.6.) Let $X(t)$ be a homogeneous Poisson process with parameter $\lambda$. Determine the covariance between $X(t)$ and $X(t + \tau)$ where $t > 0$ and $\tau > 0$, i.e. compute

$$
\mathbb{E} \left[ \left( X(t) - \mathbb{E}(X(t)) \right) \left( X(t+\tau) - \mathbb{E}(X(t+\tau)) \right) \right]
$$



(4.9.) Let $X(t)$ be a pure birth continuous time Markov chain. Assume that 

$$
\begin{aligned}
P(event \in (t, th) | X(t) = odd) &= \lambda_1 h + o(h) \\[8pt]
P(event \in (t, th) | X(t) = odd) &= \lambda_1 h + o(h) \\[8pt]
\end{aligned}
$$


For the single-server process with $\lambda < \mu$ the stationary distribution is 

$$
\pi_n = \frac{\lambda_0 \lambda_1 \cdots \lambda_{n-1} }{\mu_1 \mu_2 \cdots \mu_n} = (\frac{\lambda}{\mu})^n
$$

which, when normalized, results in

$$
P_n = \frac{\mu - \lambda}{\mu} (\frac{\lambda}{\mu})^n, \ \ n \ge 0
$$


If the process has been going on a long time and $\lambda < \mu$, the probability of being served immediately upon arrival is 

$$
P_0 = (1 - \frac{\lambda}{\mu})
$$


If an arriving customer finds $n$ people in front of her, her total waiting time $T$, including his own service time, is the sum of service times of herself and those ahead, all distributed exponentially with param $\mu$, thus 

$$
\begin{aligned}
T | n \ \ \mathrm{ahead} &\equiv Gamma(n+1, \mu) \\[8pt]
\Leftrightarrow \ \ P \{ T \le t | n \ \ \mathrm{ahead}  \} &= \int_0^t \frac{\mu^{n+1} \tau^n e^{-\mu t} }{\Gamma(n+1)} d\tau \\[10pt]
\therefore \ \ P \{ T \le t\} &= \sum_{n=0}^\infty P \{ T \le t | n \ \ \mathrm{ahead} \} \cdot (\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})
\end{aligned}
$$  

since $(\frac{\lambda}{\mu})^n (1 - \frac{\lambda}{\mu})$ is the probability that in the stationary case a customer on arrival will find $n$ ahead in line. 


$$
\begin{aligned}
P \{ T \le t\} &= \\[8pt]
\end{aligned}
$$



$$
\begin{aligned}
M(t) &= \sum_{j=0}^\infty j P_{ij}(t) \\[8pt]
M'(t) &= \lambda - \mu M(t) \\[8pt]
M(t) &= \frac{\lambda}{\mu} (1 - e^{-\mu t}) + i e^{-\mu t} \\[8pt]
\end{aligned}
$$

If we let $t \to \infty$, then $M(t) \to \lambda/\mu$, which is the mean value of the stationary distribution given above. 





\end{document}